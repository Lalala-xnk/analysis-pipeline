# -*- coding: utf-8 -*-
"""Analysis_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b-zcPObBHzOmmOuchmdH6Txgkto6tX1g

# Coreference Resolution

Configuration
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/github/analysis-pipeline/coref

import os
os.environ['data_dir'] = "."
os.environ['CHOSEN_MODEL'] = 'spanbert_base'
! pip uninstall -y tensorflow
! pip install -r requirements.txt -q
! chmod u+x setup_all.sh
! ./setup_all.sh

"""Input"""

import pandas as pd

original_text = '''
SINGAPORE: Singapore Press Holdings (SPH) said on Tuesday (Mar 22) its shareholders had approved acquisition by Cuscaden Peak, after it announced it was terminating rival bidder Keppel Corp's offer.

Both Cuscaden Peak and Keppel Corp are linked to state investor Temasek, and the deal had sparked a rare bidding war between the two groups, with the former offering S$2.40 per share, or S$3.9 billion, and the latter offering S$2.351 per share or S$3.74 billion for SPH's real estate business.

Keppel Corp, which counts Temasek Holdings as a major shareholder, has previously said it did not agree with SPH's move to terminate its offer. The conglomerate has filed an arbitration notice with the Singapore International Arbitration Centre.

Cuscaden Peak, a consortium of property tycoon Ong Beng Seng's Hotel Properties and two independently managed portfolio companies of Temasek, clinched the acquisition, SPH said in a statement.

It will gain access to SPH's real estate portfolio, which includes malls, residential properties, student accommodation and nursing homes, but not its loss-making media business, which was separated from the company last year.

A Singapore court is expected to approve the Cuscaden takeover around Apr 5, and shareholders can expect to start receiving payments from May 11, SPH said.

SPH has also been granted permission by the Singapore Exchange to skip announcing its financial results in the first half of this year, subject to shareholder approval.
'''
keyword = 'Keppel'
genre = 'nw'
model_name = 'spanbert_base'

"""Preprocess"""

import nltk
from bert import tokenization
import json
nltk.download('punkt')


original_text = original_text
text = nltk.tokenize.sent_tokenize(original_text.replace('\n', ' '))
cnt = 0
sentence_dict = []
for (i, sentence) in enumerate(text):
    cnt += len(sentence.split())
    sentence_dict.append(cnt)

data = {
    'doc_key': genre,
    'sentences': [["[CLS]"]],
    'speakers': [["[SPL]"]],
    'clusters': [],
    'sentence_map': [0],
    'subtoken_map': [0],
}

# Determine Max Segment
max_segment = None
for line in open('experiments.conf'):
    if line.startswith(model_name):
        max_segment = True
    elif line.strip().startswith("max_segment_len"):
        if max_segment:
            max_segment = int(line.strip().split()[-1])
            break

tokenizer = tokenization.FullTokenizer(vocab_file="cased_config_vocab/vocab.txt", do_lower_case=False)
subtoken_num = 0
for sent_num, line in enumerate(text):
    raw_tokens = line.split()
    tokens = tokenizer.tokenize(line)
    if len(tokens) + len(data['sentences'][-1]) >= max_segment:
        data['sentences'][-1].append("[SEP]")
        data['sentences'].append(["[CLS]"])
        data['speakers'][-1].append("[SPL]")
        data['speakers'].append(["[SPL]"])
        data['sentence_map'].append(sent_num - 1)
        data['subtoken_map'].append(subtoken_num - 1)
        data['sentence_map'].append(sent_num)
        data['subtoken_map'].append(subtoken_num)

    ctoken = raw_tokens[0]
    cpos = 0
    for token in tokens:
        data['sentences'][-1].append(token)
        data['speakers'][-1].append("-")
        data['sentence_map'].append(sent_num)
        data['subtoken_map'].append(subtoken_num)

        if token.startswith("##"):
            token = token[2:]
        if len(ctoken) == len(token):
            subtoken_num += 1
            cpos += 1
            if cpos < len(raw_tokens):
                ctoken = raw_tokens[cpos]
        else:
            ctoken = ctoken[len(token):]

data['sentences'][-1].append("[SEP]")
data['speakers'][-1].append("[SPL]")
data['sentence_map'].append(sent_num - 1)
data['subtoken_map'].append(subtoken_num - 1)

with open("in.json", 'w') as out:
    json.dump(data, out, sort_keys=True)

! cat in.json

"""Predict"""

! GPU=0 python predict.py $CHOSEN_MODEL in.json out.txt

"""Postprocess"""

output = json.load(open("out.txt"))

comb_text = [word for sentence in output['sentences'] for word in sentence]

def convert_cluster(mention):
    end = output['subtoken_map'][mention[1]] + 1
    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(" ##"))
    return (end, mtext)

seen = set()
clusters = []
clusters_idx = []
for cluster in output['predicted_clusters']:
    mapped = []
    mapped_idx = []
    for mention in cluster:
        seen.add(tuple(mention))
        convert = convert_cluster(mention)
        mapped.append(convert[1])
        mapped_idx.append(convert[0])
    clusters.append(mapped)
    clusters_idx.append(mapped_idx)

idx = []
for i, cluster in enumerate(clusters):
    for item in cluster:
        if keyword.lower() in item.lower():
            idx.append(i)
            break

print('Relevant sentences for \'' + keyword + '\':')
relevant_sentences = []
if len(idx):
    relevant_idx = []
    for i in idx:
        sentences_idx = []
        start_idx = 0
        for word_idx in clusters_idx[i]:
            while word_idx > sentence_dict[start_idx]:
                start_idx += 1
            sentences_idx.append(start_idx)
        relevant_idx.extend(sentences_idx)
    for i in set(relevant_idx):
        print(text[i])
        relevant_sentences.append(text[i])
else:
    print('None')
out_df = pd.DataFrame(relevant_sentences)
out_df.to_csv('coref_out.csv', index=False, header=None)

"""# Aspect Category Detection

Configuration
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/github/analysis-pipeline

"""Input"""

import pandas as pd
import numpy as np

label_list = ['Yes', 'No']

context_id_map_fiqa = {"legal": 0, "m&a": 1, "regulatory": 2, "risks": 3, "rumors": 4, "company communication": 5, "trade": 6, "central banks": 7, "market": 8, "volatility": 9, "financial": 10, "fundamentals": 11, "price action": 12, "insider activity": 13, "ipo": 14, "others": 15}

df_original = pd.read_csv('./coref/coref_out.csv', header=None)

"""Preprocess"""

data = {'context': list(context_id_map_fiqa.keys()) * len(df_original), 'text': list(np.repeat(df_original.values, 16))}
df_processed = pd.DataFrame(data)
df_processed.to_csv('acd_in.csv', index=None, header=None)

"""Predict"""

! python ./absa/aspect_detector.py --path ./acd_in.csv --vocab_file ./absa/vocab.txt --bert_config_file ./absa/bert_config.json --init_checkpoint ./absa/acd_best_checkpoint.bin

"""# Aspect-based Sentiment Analysis

Configuration
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/github/analysis-pipeline

"""Predict"""

! python ./absa/scorer.py --path ./acd_out.csv --vocab_file ./absa/vocab.txt --bert_config_file ./absa/bert_config.json --init_checkpoint ./absa/absa_best_checkpoint.bin

"""Postprocess"""

df_absa_out = pd.read_csv('./absa_out.csv', header=None)
aspects = ["legal", "m&a", "regulatory", "risks", "rumors", "company communication", "trade", "central banks", 
           "market", "volatility", "financial", "fundamentals", "price action", "insider activity", "ipo", "others"]
sentiment_scores = {}
for aspect in aspects:
    tmp = df_absa_out.where(df_absa_out.iloc[:, 1] == aspect).mean(numeric_only=True).values[0]
    if tmp > 0 or tmp < 0:
        print(aspect, tmp)
        sentiment_scores[aspect] = tmp
